#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Requires PyYAML <http://pyyaml.org/>
import yaml
import csvkit
import os
import errno
import glob
import json
import argparse
import urllib2
import zipfile
import sys

parser = argparse.ArgumentParser(
    description="A parser for Virginia State Corporation Commission records",
    epilog="https://github.com/openva/crump/")
parser.add_argument('-a', '--atomize', help="generate millions of per-record JSON files", action='store_true')
parser.add_argument('-i', '--input', default='cisbemon.txt', help="raw SCC data (default: %(default)s)", metavar="file.txt")
parser.add_argument('-o', '--output', default='output', help="directory for JSON and CSV", metavar="output_dir")
parser.add_argument('-t', '--transform', help="format properly date, ZIP, etc. fields", action='store_true')
parser.add_argument('-d', '--download', help="download the data file, if missing", action='store_true')
parser.add_argument('-e', '--elasticsearch', help="create Elasticsearch bulk API data", action='store_true')
args = parser.parse_args()
master_file = args.input
output_dir = args.output
atomize = args.atomize
transform = args.transform
download = args.download
elasticsearch = args.elasticsearch

def main():
    
    # Make sure the master file exists
    if os.path.isfile(master_file) is False:
        
        if transform:
			# If it doesn't exist, download it.
			from urllib2 import Request, urlopen, URLError, HTTPError
			url = 'https://s3.amazonaws.com/virginia-business/current.zip'
			req = Request(url)
	
			try:
				f=urlopen(req)
				print "Downloading data from " + url
				local_file = open('current.zip', 'wb')
				local_file.write(f.read())
				local_file.close()
		
			except HTTPError as e:
				print "HTTP error " + format(e.code) + ": Could not download " + url
				sys.exit()
		
			# Uncompress the ZIP file.
			archive = zipfile.ZipFile("current.zip")
			archive.extract('cisbemon.txt')
		
        else:
		    print "Error: cisbemon.txt data file not found"
		    sys.exit()
    
    # Create the output directory.
    try:
        os.makedirs(output_dir)
    except OSError as exception:
        if exception.errno != errno.EEXIST:
            raise

    field_maps = {}

    for csv_file in glob.glob("table_maps/*.yaml"):
        stream = open(csv_file, 'r')

        # Import the data from YAML.
        field_map = yaml.load(stream)

        head, tail = os.path.split(csv_file)

        map_id = tail[0]
        field_maps[map_id] = {}
        field_maps[map_id]["map"] = field_map
        field_maps[map_id]["name"] = tail

    # Count the number of lines in the master file.
    last_file = ""
    current_map = None
    csv_writer = None
    json_file = None
    with open(master_file) as f:
        for current_line in enumerate(f):
        	
        	# The file number is the first character on each line.
            file_number = current_line[1][1]
            
            # If we're outputting millions of JSON files, make sure the directory exists.
            if atomize:
                dir = "output/" + file_number + "/"
                try:
                    os.stat(dir)
                except:
                    os.mkdir(dir)
                
            if file_number in field_maps:
                
                # If we've begun a new file.
                if file_number != last_file:
                
                    # If we've just finished with file 1, read it into memory to use elsewhere.
                    if last_file == '1':
                        table_1 = csvkit.CSVKitDictReader(open("output/1_tables.csv", 'rb'))
                        lookup_table = []
                        for row in table_1:
                            lookup_table.append(row)
                    
                    # Terminate the prior JSON file: remove the trailing comma and add a bracket.
                    if last_file != "":
                        json_file.seek(-2, os.SEEK_END)
                        json_file.truncate()
                        json_file.write(']')
                   
                    # This is a new file: retrieve the correct field map and set up the file.
                    current_map = field_maps[file_number]["map"]
                    current_name = field_maps[file_number]["name"]
                    csv_name = current_name.replace(".yaml", ".csv")
                    json_name = current_name.replace(".yaml", ".json")
                    last_file = file_number
                    csv_file = open("output/"+csv_name, 'wb')
                    json_file = open("output/"+json_name, 'wb')
                    field_names = []
                    for field in current_map:
                        field_names.append(field["name"])
                    field_tuple = tuple(field for field in field_names)
                    
                    # Start writing the CSV data.
                    csv_writer = csvkit.DictWriter(csv_file, field_tuple)
                    csv_writer.writeheader()
                    print "Creating",csv_name.replace(".csv","")
                    
                    # Start a new JSON file with an opening bracket.
                    json_file.write('[')
                
                # Break the line up into pieces.
                line = {}
                for field in current_map:
                    start = int(field["start"])
                    length = int(field["length"])
                    name = field["name"]
                    end = start + length
                    if 'table_id' in field:
					    table_id = field["table_id"]
                    else:
					    table_id = None
                    line[name] = current_line[1][start:end].strip()
                    if "corp-id" in name:
                        corp_id = line[name]
                    
                    # If we have elected, via command-line option, to transform field contents.
                    if transform:
                    	
                    	# Format dates properly.
                        if "date" in name:
                            line[name] = line[name][:4] + '-' + line[name][4:-2] + '-' + line[name][-2:]
                            if line[name] == "0000-00-00":
                                line[name] = ''
                        
                        # Format ZIP codes properly.
                        if "zip" in name:
                            if len(line[name]) == 9:
                        	    if line[name] == '000000000':
                        		    line[name] = ''
                        	    if line[name][-4:] == '0000':
                        	        line[name] = line[name][:-4]
                        	    else:
                        	    	line[name] = line[name][:-4] + '-' + line[name][-4:]
                        
                        # Replace shorthand values with the full version, from the lookup table.
                        if table_id != None:
                            for index, conversion in enumerate(lookup_table):
                                if int(conversion["table-identifier"]) == table_id:
                                    if conversion["table-code"] == line[name]:
                                        line[name] = conversion["table-desc"]
                                        break
                        		    
                try:
                    csv_writer.writerow(line)
                except UnicodeDecodeError as exception:
                    print "Found an incorrect character in",line,"and fixed it, hopefully."
                    for key in line:
                        line[key] = remove_non_ascii(line[key])
                    csv_writer.writerow(line)
                
                # If we’re creating one JSON file for each individual record.
                if atomize:
                    try:
                        corp_id
                    except NameError:
                        pass
                    else:
                        entity_json_file = open("output/" + file_number + "/" + corp_id + ".json", 'wb')
                        json.dump(line,entity_json_file);
                
                # If we’re creating Elasticsearch bulk API import files.
                if elasticsearch:
                    try:
                        corp_id
                    except NameError:
                        pass
                    else:
						index = dict()
						index['index'] = dict()
						index['index']['_index'] = 'finance'
						index['index']['_type'] = file_number
						index['index']['_id'] = corp_id
						json.dump(index,json_file)
						json_file.write(',\n')
                
                # Save the JSON data.
                json.dump(line,json_file)
                
                # Add a separating comma between elements.
                json_file.write(',\n')
     
    # Now that all files are output, break them into Elasticsearch-sized chunks.
    if elasticsearch:
        for json_file in glob.glob(output_dir + "/*.json"):
            chunk(os.path.basename(json_file))

# From http://stackoverflow.com/a/1342373/3579517 with slight modification to replace with space
def remove_non_ascii(s):
    
    return "".join(i if ord(i)<128 else " " for i in s )

# Split a file into smaller chunks.
def chunk(filename):

    # This must be an even number to avoid separating Elasticsearch metadata from actual data.
    max_lines = 100000
    
    # Ensure that the specified filename exists.
    if os.path.isfile(output_dir + '/' + filename) is False:
    	return False
    
    print "Breaking " + output_dir + '/' + filename + " into pieces"
    
    # Count the number of lines in the file.
    num_lines = sum(1 for line in open(output_dir + '/' + filename))
    
    # Only bother to break up files with more than 100,000 lines.
    if num_lines < max_lines:
        return True
    
    i = 0
    j = 1
    source_file = open(output_dir + '/' + filename)
    dest_name = os.path.splitext(filename)[0] + '_' + str(j) + '.json'
    dest_file = open(output_dir + '/' + dest_name, 'wb')
    for line in source_file:
        dest_file.write(line)
        i += 1
        if i == max_lines:
            j += 1
            i=0
            
            # Wrap up the current file, hacking off the trailing comma and adding a closing bracket.
            dest_file.seek(-2, os.SEEK_END)
            dest_file.truncate()
            dest_file.write(']')
            dest_file.close()
            
            # Start a new file.
            dest_name = os.path.splitext(filename)[0] + '_' + str(j) + '.json'
            dest_file = open(output_dir + '/' + dest_name, 'wb')
            dest_file.write('[')
    
    source_file.close()
    
    # Delete the original JSON file, now that we have it in pieces.
    os.remove(output_dir + '/' + filename)

# Calculate the checksum digit for a given corporate ID
def checksum(corp_id):
    
    if len(corp_id) != 7:
        return False
    
    if corp_id[:1].isalpha():
        corp_id = '8' + corp_id[1:]
    
    # Rearrange the digits
    corp_id = corp_id[:1] + corp_id[6:7] + corp_id[1:-1]
    
    # Add up the odd digits.
    checksum = int(corp_id[:1]) + int(corp_id[2:3]) + int(corp_id[4:5]) + int(corp_id[6:7]);
    
    # Multiple each even digit by two; if the result is 2 digits, add them to each other.
    pos = [2, 4, 6]
    for even in pos:
	    tmp = int(corp_id[even-1:even]) * 2
	    if tmp > 9:
	        tmp = int(tmp[:1]) + int(tmp[:-1])
	    checksum = int(checksum) + tmp
	
	# Subtract the number from 100 and return the least significant digit.
    checksum = 100 - checksum
    checksum = str(checksum)
    checksum = checksum[1:2]
	
    return checksum

if __name__ == "__main__":
    main()
