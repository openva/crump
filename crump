#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Requires PyYAML <http://pyyaml.org/>
import yaml
import csvkit
import os
import errno
import glob
import json
import argparse
import zipfile
import sys
import hashlib
import sqlite3
import re

parser = argparse.ArgumentParser(
    description="A parser for Virginia State Corporation Commission records",
    epilog="https://github.com/openva/crump/")
parser.add_argument('-a', '--atomize', help="generate millions of per-record JSON files", action='store_true')
parser.add_argument('-i', '--input', default='cisbemon.txt', help="raw SCC data (default: %(default)s)", metavar="file.txt")
parser.add_argument('-o', '--output', default='output', help="directory for JSON and CSV", metavar="output_dir")
parser.add_argument('-t', '--transform', help="format properly the date, ZIP, etc. fields", action='store_true')
parser.add_argument('-d', '--download', help="download the data file, if missing", action='store_true')
parser.add_argument('-e', '--elasticsearch', help="create Elasticsearch bulk API data", action='store_true')
parser.add_argument('-m', '--map', help="generate Elasticsearch index map", action='store_true')
args = parser.parse_args()
master_file = args.input
output_dir = args.output
atomize = args.atomize
transform = args.transform
download = args.download
elasticsearch = args.elasticsearch
elasticsearch_map = args.map

def main():

    # Make sure the master file exists
    if os.path.isfile(master_file) is False:
        
        # If it doesn't exist, download it.
        if download:
            
            from urllib2 import Request, urlopen, HTTPError
            url = 'https://s3.amazonaws.com/virginia-business/current.zip'
            req = Request(url)

            try:
                f = urlopen(req)
                print "Downloading data from " + url
                local_file = open('current.zip', 'w')
                local_file.write(f.read())
                local_file.close()

            except HTTPError as e:
                print "HTTP error " + format(e.code) + ": Could not download " + url
                sys.exit()

            # Uncompress the ZIP file.
            archive = zipfile.ZipFile("current.zip")
            archive.extract('cisbemon.txt')

        else:
            print "Error: cisbemon.txt data file not found"
            sys.exit()

        # If we're going to be transforming the data, retrieve (if necessary) and connect to our
        # database of geocoded addresses.
        if transform:
        
            # Connect to the database of geocoded addresses.
            try:
                db = sqlite3.connect('addresses.db')
            except sqlite3.error, e:
                print "Count not connect to SQLite, with error %s:" % e.args[0]
                sys.exit(1)

            # Create a SQLite cursor.
            cursor = db.cursor()

            # See if the addresses table already exists.
            cursor.execute("SELECT 1 FROM sqlite_master WHERE type='table' AND name='addresses'")
            exists = cursor.fetchone()

            # If the addresses table does not exist, try to retrieve it from a remote cache.
            if exists is None:
                 db.close()
                 from urllib2 import Request, urlopen, HTTPError
                 url = 'https://s3.amazonaws.com/virginia-business/addresses.db'
                 req = Request(url)

                 try:
                     f = urlopen(req)
                     print "Downloading data from " + url
                     local_file = open('current.zip', 'w')
                     local_file.write(f.read())
                     local_file.close()

                     # Now that we have it, reconnect to the database of geocoded addresses.
                     db = sqlite3.connect('addresses.db')
                     cursor = db.cursor()

                 except:
                     pass

    # Create the output directory.
    try:
        os.makedirs(output_dir)
    except OSError as exception:
        if exception.errno != errno.EEXIST:
            raise

    field_maps = {}

    for csv_file in glob.glob("table_maps/*.yaml"):
        stream = open(csv_file, 'r')

        # Import the data from YAML.
        field_map = yaml.load(stream)

        head, tail = os.path.split(csv_file)

        map_id = tail[0]
        field_maps[map_id] = {}
        field_maps[map_id]["map"] = field_map
        field_maps[map_id]["name"] = tail

    # If an Elasticsearch mapping has been requested via command-line switch.
    if elasticsearch_map:
        print "Writing Elasticsearch index maps to " + output_dir + "/elasticsearch_maps/"
        search_index = dict()

        # Create the directory in which they'll be stored.
        try:
            os.makedirs(output_dir + "/elasticsearch_maps/")
        except OSError as exception:
            if exception.errno != errno.EEXIST:
                raise

        # Iterate through every field and build up our index data.
        for file_number in field_maps:

            search_index[file_number] = dict()
            search_index[file_number]['properties'] = dict()

            current_map = field_maps[file_number]['map']
            
            # Iterate through every field in this mapping file.
            for map in current_map:

                try:
                    map['alt_name']
                except KeyError:
                    continue

                name = map['alt_name']
                search_index[file_number]['properties'][name] = dict()

                # If this field contains Elasticsearch instructions, we want to handle those.
                try:
                    map['search']
                except KeyError:
                    search_index[file_number]['properties'][name]['type'] = 'string'
                else:

                    try:
                        map['search']['type']
                    except KeyError:
                        search_index[file_number]['properties'][name]['type'] = 'string'
                    else:
                        if map['search']['type'] == "geo_shape":
                            search_index[file_number]['properties']['location'] = {}
                            search_index[file_number]['properties']['location']['type'] = 'geo_shape'
                            del search_index[file_number]['properties'][name]
                        else:
                            search_index[file_number]['properties'][name]['type'] = map['search']['type']

                    try:
                        map['search']['match']
                    except KeyError:
                        pass
                    else:
                        if map['search']['match'] == 'exact':
                            search_index[file_number]['properties'][name]['index'] = 'not_analyzed'

            file = open(output_dir + "/elasticsearch_maps/" + file_number + ".json", 'w')
            file.write(json.dumps(search_index[file_number], indent=4))
            file.close()

    # Include our character replacement table.
    stream = open("character_map.yaml", 'r')
    global character_map
    character_map = yaml.load(stream)

    # Count the number of lines in the master file.
    last_file = ""
    current_map = None
    csv_writer = None
    json_file = None
    
    with open(master_file) as f:

        # If we're transforming data, and thus adding geodata, connect to our address database.
        if transform:
            db = sqlite3.connect('addresses.db')
            cursor = db.cursor()
        
        # Iterate through our master file and start converting the data.
        for current_line in enumerate(f):

            # The file number is the first character on each line.
            file_number = current_line[1][1]

            # If we're outputting millions of JSON files, make sure the directory exists.
            if atomize:
                dir = output_dir + '/' + file_number + '/'
                try:
                    os.stat(dir)
                except:
                    os.mkdir(dir)

            if file_number in field_maps:

                # If we've begun a new file.
                if file_number != last_file:

                    # If we've just finished with file 1, read it into memory to use elsewhere.
                    if last_file == '1':
                        table_1 = csvkit.CSVKitDictReader(open(output_dir + "/1_tables.csv", 'rb'))
                        lookup_table = []
                        for row in table_1:
                            lookup_table.append(row)

                    # Terminate the prior JSON file: remove the trailing comma and add a bracket,
                    # unless this is for Elasticsearch.
                    if last_file != "":
                        json_file.seek(-2, os.SEEK_END)
                        json_file.truncate()
                        if elasticsearch == False:
                            json_file.write(']')

                    # This is a new file: retrieve the correct field map and set up the file.
                    current_map = field_maps[file_number]["map"]
                    current_name = field_maps[file_number]["name"]
                    csv_name = current_name.replace(".yaml", ".csv")
                    json_name = current_name.replace(".yaml", ".json")
                    last_file = file_number
                    csv_file = open(output_dir + "/" + csv_name, 'w')
                    json_file = open(output_dir + "/" + json_name, 'w')
                    field_names = []
                    for field in current_map:
                        # Optionally substitute cleaner field names.
                        if transform and 'alt_name' in field.keys():
                            field_names.append(field["alt_name"])
                        else:
                            field_names.append(field["name"])
                    field_tuple = tuple(field for field in field_names)

                    # Start writing the CSV data.
                    csv_writer = csvkit.DictWriter(csv_file, field_tuple)
                    csv_writer.writeheader()
                    print "Creating", csv_name.replace(".csv", "")

                    # Start a new JSON file with an opening bracket, unless this is for
                    # Elasticsearch data.
                    if elasticsearch == False:
                        json_file.write('[')

                # Break the line up into pieces.
                line = {}
                for field in current_map:
                    # Skip any fields that lack start data -- those are columns that don't exist in
                    # the master file, but that are added during the parsing process, and are
                    # included in the map in order to have them indexed by Elasticsearch.
                    if 'start' in field:
                        pass
                    else:
                        continue
                    start = int(field["start"])
                    length = int(field["length"])    
                                    
                    # Optionally substitute cleaner field names.
                    if transform and 'alt_name' in field.keys():
                        name = field["alt_name"]
                    else:
                        name = field["name"]
                    end = start + length
                    if 'table_id' in field:
                        table_id = field["table_id"]
                    else:
                        table_id = None
                    line[name] = current_line[1][start:end].strip()
                    if "corp-id" in name:
                        corp_id = line[name]

                    # If we have elected, via command-line option, to transform field contents.
                    if transform:

                        # Format dates properly.
                        if "date" in name:
                            line[name] = line[name][:4] + '-' + line[name][4:-2] + '-' + line[name][-2:]
                            if line[name] == "0000-00-00":
                                line[name] = None
                            elif line[name] == "9999-99-99":
                                line[name] = None

                        # Format ZIP codes properly.
                        elif "zip" in name:
                            if len(line[name]) == 9:
                                if line[name] == '000000000':
                                    line[name] = ''
                                if line[name][-4:] == '0000':
                                    line[name] = line[name][:-4]
                                else:
                                    line[name] = line[name][:-4] + '-' + line[name][-4:]

                        # Indicate whether a corporation is foreign or domestic, using True/False
                        # values instead of the F/0 values the SCC uses in 2_corporate.
                        elif name == 'foreign':
                            if line[name] == '0':
                                line[name] = False
                            else:
                                line[name] = True

                        # Indicate whether a corporation is foreign or domestic, using True/False
                        # values instead of the M/L values the SCC uses in 3_lp.
                        elif name == 'domestic':
                            if line[name] == 'M':
                                line[name] = False
                            else:
                                line[name] = True

                        # Remove extraneous internal whitespace.
                        if type(line[name]) is str:
                            line[name] = re.sub("\s{2,}", " ", line[name])

                        # Replace shorthand values with the full version, from the lookup table.
                        if table_id != None:
                            for index, conversion in enumerate(lookup_table):
                                if int(conversion["table-identifier"]) == table_id:
                                    if conversion["table-code"] == line[name]:
                                        line[name] = conversion["table-desc"]
                                        break

                # If we have geodata to be inserted.
                if transform:
                    #if "group" in current_map:
                    #and we have a group with "address" in its name and it has a member of
                    #search > type with the value "geo_point":
                    if file_number == '2' or file_number == '3' or file_number == '9':
                    
                        # Collapse our address columns and save that as a hash, to query the
                        # database for. Convert to a byte str to avoid fatal Unicode-related erors
                        # for the odd problematic character.
                        try:
                            address_hash = hashlib.md5(line['street_1'] + "," + line['street_2'] \
                                + "," + line['city'] + "," + line['state'] + "," + line['zip']).hexdigest()

                        # In the very rare instance of a character encoding error, fix the error
                        # and try again.
                        except UnicodeEncodeError as exception:
                            for key in line:
                                if line[key] != None:
                                    line[key] = replace_non_ascii(line[key])
                            address_hash = hashlib.md5(line['street_1'] + "," + line['street_2'] \
                                + "," + line['city'] + "," + line['state'] + "," + line['zip']).hexdigest()

                        else:
                            # If the hashed, combined string is already stored in the database, then
                            # we have coordinates for it.
                            hash_param = (address_hash,)
                            cursor.execute("SELECT latitude, longitude FROM addresses WHERE address_hash = ?", hash_param)
                            coordinates = cursor.fetchone()
                            if coordinates is not None:
                                # FIX THE BELOW -- we should not be naming "coordinates" manually,
                                # but instead using the identified field name.
                                line['coordinates'] = [coordinates[1], coordinates[0]]

                try:
                    csv_writer.writerow(line)
                except UnicodeDecodeError as exception:
                    for key in line:
                        if line[key] != None:
                            line[key] = replace_non_ascii(line[key])
                    csv_writer.writerow(line)

                # If we’re creating one JSON file for each individual record.
                if atomize:
                    try:
                        corp_id
                    except NameError:
                        pass
                    else:
                        entity_json_file = open(output_dir + "/" + file_number + "/" + corp_id + ".json", 'w')
                        json.dump(line, entity_json_file)

                # If we’re creating Elasticsearch bulk API import files.
                if elasticsearch:
                    index = dict()
                    index['index'] = dict()
                    index['index']['_index'] = 'business'
                    index['index']['_type'] = file_number
                    json.dump(index, json_file)
                    json_file.write(',\n')

                # Make a few modifications to the data before writing it as JSON. (CSV only has
                # columns, while of course JSON can have nested data, so we take this opportunity
                # to nest some data.)
                if transform:
                    if 'coordinates' in line:
                        line['location'] = dict()
                        line['location']['coordinates'] = line['coordinates']
                        line['location']['type'] = 'point'
                        del line['coordinates']

                # Save the JSON data.
                json.dump(line, json_file)

                # Add a separating comma between elements.
                if elasticsearch:
                   json_file.write('\n')
                else:
                   json_file.write(',\n')

    # Now that all files are output, break them into Elasticsearch-sized chunks.
    if elasticsearch:
        for json_file in glob.glob(output_dir + "/*.json"):
            chunk(os.path.basename(json_file))

# Replace invalid non-ASCII characters with the correct character.
def replace_non_ascii(s):
    if isinstance(s, str):
        o = ''
        for i in s:
            if ord(i) > 128:
                key = hex(ord(i))
                try:
                    replacement = character_map[key[2:]]
                except KeyError:
                    o += ' '
                else:
                    o += replacement
            else:
                o += i
    else:
        o = s
    return o

# Split a file into smaller chunks.
def chunk(filename):

    # This must be an even number to avoid separating Elasticsearch metadata from actual data.
    max_lines = 100000

    # Ensure that the specified filename exists.
    if os.path.isfile(output_dir + '/' + filename) is False:
        return False

    print "Breaking " + output_dir + '/' + filename + " into pieces"

    # Count the number of lines in the file.
    num_lines = sum(1 for line in open(output_dir + '/' + filename))

    # Only bother to break up files with more than 100,000 lines.
    if num_lines < max_lines:
        return True

    i = 0
    j = 1
    source_file = open(output_dir + '/' + filename)
    dest_name = os.path.splitext(filename)[0] + '_' + str(j) + '.json'
    dest_file = open(output_dir + '/' + dest_name, 'w')
    for line in source_file:
        dest_file.write(line)
        i += 1
        if i == max_lines:
            j += 1
            i = 0

            # Wrap up the current file, hacking off the trailing comma and adding a closing bracket.
            dest_file.seek(-2, os.SEEK_END)
            dest_file.truncate()
            dest_file.write(']')
            dest_file.close()

            # Start a new file.
            dest_name = os.path.splitext(filename)[0] + '_' + str(j) + '.json'
            dest_file = open(output_dir + '/' + dest_name, 'w')
            dest_file.write('[')

    source_file.close()

    # Delete the original JSON file, now that we have it in pieces.
    os.remove(output_dir + '/' + filename)

# Calculate the checksum digit for a given corporate ID
def checksum(corp_id):

    if len(corp_id) != 7:
        return False

    if corp_id[:1].isalpha():
        corp_id = '8' + corp_id[1:]

    # Rearrange the digits
    corp_id = corp_id[:1] + corp_id[6:7] + corp_id[1:-1]

    # Add up the odd digits.
    checksum = int(corp_id[:1]) + int(corp_id[2:3]) + int(corp_id[4:5]) + int(corp_id[6:7])

    # Multiple each even digit by two; if the result is 2 digits, add them to each other.
    pos = [2, 4, 6]
    for even in pos:
        tmp = int(corp_id[even-1:even]) * 2
        if tmp > 9:
            tmp = int(tmp[:1]) + int(tmp[:-1])
        checksum = int(checksum) + tmp

    # Subtract the number from 100 and return the least significant digit.
    checksum = 100 - checksum
    checksum = str(checksum)
    checksum = checksum[1:2]

    return checksum

if __name__ == "__main__":
    main()
