#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Requires PyYAML <http://pyyaml.org/>
import yaml
import csvkit
import os
import errno
import glob
import json
import argparse
import urllib2
import zipfile
import sys

parser = argparse.ArgumentParser(
    description="A parser for Virginia State Corporation Commission records",
    epilog="https://github.com/openva/crump/")
parser.add_argument('-a', '--atomize', help="generate millions of per-record JSON files", action='store_true')
parser.add_argument('-i', '--input', default='cisbemon.txt', help="raw SCC data (default: %(default)s)", metavar="file.txt")
parser.add_argument('-o', '--output', default='output', help="directory for JSON and CSV", metavar="output_dir")
parser.add_argument('-t', '--transform', help="format properly the date, ZIP, etc. fields", action='store_true')
parser.add_argument('-d', '--download', help="download the data file, if missing", action='store_true')
parser.add_argument('-e', '--elasticsearch', help="create Elasticsearch bulk API data", action='store_true')
parser.add_argument('-m', '--map', help="generate Elasticsearch index map", action='store_true')
args = parser.parse_args()
master_file = args.input
output_dir = args.output
atomize = args.atomize
transform = args.transform
download = args.download
elasticsearch = args.elasticsearch
elasticsearch_map = args.map

def main():
    
    # Make sure the master file exists
    if os.path.isfile(master_file) is False:
        
        if transform:
            # If it doesn't exist, download it.
            from urllib2 import Request, urlopen, URLError, HTTPError
            url = 'https://s3.amazonaws.com/virginia-business/current.zip'
            req = Request(url)
    
            try:
                f=urlopen(req)
                print "Downloading data from " + url
                local_file = open('current.zip', 'w')
                local_file.write(f.read())
                local_file.close()
        
            except HTTPError as e:
                print "HTTP error " + format(e.code) + ": Could not download " + url
                sys.exit()
        
            # Uncompress the ZIP file.
            archive = zipfile.ZipFile("current.zip")
            archive.extract('cisbemon.txt')
        
        else:
            print "Error: cisbemon.txt data file not found"
            sys.exit()
    
    # Create the output directory.
    try:
        os.makedirs(output_dir)
    except OSError as exception:
        if exception.errno != errno.EEXIST:
            raise

    field_maps = {}

    for csv_file in glob.glob("table_maps/*.yaml"):
        stream = open(csv_file, 'r')

        # Import the data from YAML.
        field_map = yaml.load(stream)

        head, tail = os.path.split(csv_file)

        map_id = tail[0]
        field_maps[map_id] = {}
        field_maps[map_id]["map"] = field_map
        field_maps[map_id]["name"] = tail
    
    
    # If an Elasticsearch mapping has been requested via command-line switch.
    if elasticsearch_map:
        print "Writing Elasticsearch index maps to " + output_dir + "/elasticsearch_maps/"
        search_index = dict()
        
        # Create the directory in which they'll be stored.
        try:
            os.makedirs(output_dir + "/elasticsearch_maps/")
        except OSError as exception:
            if exception.errno != errno.EEXIST:
                raise
        
        # Iterate through every field and build up our index data.
        for file_number in field_maps:
            
            search_index[file_number] = dict()
            search_index[file_number]['properties'] = dict()
            
            current_map = field_maps[file_number]['map']
            for map in current_map:
                    
                    try:
                        map['alt_name']
                    except KeyError:
                        continue
                    
                    name = map['alt_name']
                    search_index[file_number]['properties'][name] = dict()
                    
                    try:
                        map['search']
                    except KeyError:
                        search_index[file_number]['properties'][name]['type'] = 'string'
                    else:
                        
                        try:
                            map['search']['type']
                        except KeyError:
                            search_index[file_number]['properties'][name]['type'] = 'string'
                        else:
                            search_index[file_number]['properties'][name]['type'] = map['search']['type']
                        
                        try:
                            map['search']['match']
                        except KeyError:
                            pass
                        else:
                            if map['search']['match'] == 'exact':
                                search_index[file_number]['properties'][name]['index'] = 'not_analyzed'
        
            file = open(output_dir + "/elasticsearch_maps/" + file_number + ".json", 'w')
            file.write(json.dumps(search_index[file_number], indent = 4))
            file.close()
    
    # Include our character replacement table.
    stream = open("character_map.yaml", 'r')
    global character_map
    character_map = yaml.load(stream)
    
    # Count the number of lines in the master file.
    last_file = ""
    current_map = None
    csv_writer = None
    json_file = None
    with open(master_file) as f:
        for current_line in enumerate(f):
            
            # The file number is the first character on each line.
            file_number = current_line[1][1]
            
            # If we're outputting millions of JSON files, make sure the directory exists.
            if atomize:
                dir = output_dir + '/' + file_number + '/'
                try:
                    os.stat(dir)
                except:
                    os.mkdir(dir)
                
            if file_number in field_maps:
                
                # If we've begun a new file.
                if file_number != last_file:
                
                    # If we've just finished with file 1, read it into memory to use elsewhere.
                    if last_file == '1':
                        table_1 = csvkit.CSVKitDictReader(open(output_dir + "/1_tables.csv", 'rb'))
                        lookup_table = []
                        for row in table_1:
                            lookup_table.append(row)
                    
                    # Terminate the prior JSON file: remove the trailing comma and add a bracket.
                    if last_file != "":
                        json_file.seek(-2, os.SEEK_END)
                        json_file.truncate()
                        json_file.write(']')
                   
                    # This is a new file: retrieve the correct field map and set up the file.
                    current_map = field_maps[file_number]["map"]
                    current_name = field_maps[file_number]["name"]
                    csv_name = current_name.replace(".yaml", ".csv")
                    json_name = current_name.replace(".yaml", ".json")
                    last_file = file_number
                    csv_file = open(output_dir + "/" + csv_name, 'w')
                    json_file = open(output_dir + "/" + json_name, 'w')
                    field_names = []
                    for field in current_map:
                        # Optionally substitute cleaner field names.
                        default = field['name']
                        if (transform and 'alt_name' in field.keys()):
                            field_names.append(field["alt_name"])
                        else:
                            field_names.append(field["name"])
                    field_tuple = tuple(field for field in field_names)
                    
                    # Start writing the CSV data.
                    csv_writer = csvkit.DictWriter(csv_file, field_tuple)
                    csv_writer.writeheader()
                    print "Creating",csv_name.replace(".csv","")
                    
                    # Start a new JSON file with an opening bracket.
                    json_file.write('[')
                
                # Break the line up into pieces.
                line = {}
                for field in current_map:
                    start = int(field["start"])
                    length = int(field["length"])                    
                    # Optionally substitute cleaner field names.
                    if (transform and 'alt_name' in field.keys()):
                        name = field["alt_name"]
                    else:
                        name = field["name"]
                    end = start + length
                    if 'table_id' in field:
                        table_id = field["table_id"]
                    else:
                        table_id = None
                    line[name] = current_line[1][start:end].strip()
                    if "corp-id" in name:
                        corp_id = line[name]
                    
                    # If we have elected, via command-line option, to transform field contents.
                    if transform:
                        
                        # Format dates properly.
                        if "date" in name:
                            line[name] = line[name][:4] + '-' + line[name][4:-2] + '-' + line[name][-2:]
                            if line[name] == "0000-00-00":
                                line[name] = None
                        
                        # Format ZIP codes properly.
                        if "zip" in name:
                            if len(line[name]) == 9:
                                if line[name] == '000000000':
                                    line[name] = ''
                                if line[name][-4:] == '0000':
                                    line[name] = line[name][:-4]
                                else:
                                    line[name] = line[name][:-4] + '-' + line[name][-4:]
                        
                        # Replace shorthand values with the full version, from the lookup table.
                        if table_id != None:
                            for index, conversion in enumerate(lookup_table):
                                if int(conversion["table-identifier"]) == table_id:
                                    if conversion["table-code"] == line[name]:
                                        line[name] = conversion["table-desc"]
                                        break
                                    
                try:
                    csv_writer.writerow(line)
                except UnicodeDecodeError as exception:
                    for key in line:
                        if line[key] != None:
                            line[key] = replace_non_ascii(line[key])
                    csv_writer.writerow(line)
                
                # If we’re creating one JSON file for each individual record.
                if atomize:
                    try:
                        corp_id
                    except NameError:
                        pass
                    else:
                        entity_json_file = open(output_dir + "/" + file_number + "/" + corp_id + ".json", 'w')
                        json.dump(line,entity_json_file);
                
                # If we’re creating Elasticsearch bulk API import files.
                if elasticsearch:
					index = dict()
					index['index'] = dict()
					index['index']['_index'] = 'finance'
					index['index']['_type'] = file_number
					json.dump(index,json_file)
					json_file.write(',\n')
                
                # Save the JSON data.
                json.dump(line,json_file)
                
                # Add a separating comma between elements.
                json_file.write(',\n')
     
    # Now that all files are output, break them into Elasticsearch-sized chunks.
    if elasticsearch:
        for json_file in glob.glob(output_dir + "/*.json"):
            chunk(os.path.basename(json_file))

# Replace invalid non-ASCII characters with the correct character.
def replace_non_ascii(s):
    o = ''
    for i in s:
        if ord(i) > 128:
            key = hex(ord(i))
            try:
                replacement = character_map[key[2:]]
            except KeyError:
                o+=' '
            else:
                o+=replacement
        else:
            o+=i
    return o

# Split a file into smaller chunks.
def chunk(filename):

    # This must be an even number to avoid separating Elasticsearch metadata from actual data.
    max_lines = 100000
    
    # Ensure that the specified filename exists.
    if os.path.isfile(output_dir + '/' + filename) is False:
        return False
    
    print "Breaking " + output_dir + '/' + filename + " into pieces"
    
    # Count the number of lines in the file.
    num_lines = sum(1 for line in open(output_dir + '/' + filename))
    
    # Only bother to break up files with more than 100,000 lines.
    if num_lines < max_lines:
        return True
    
    i = 0
    j = 1
    source_file = open(output_dir + '/' + filename)
    dest_name = os.path.splitext(filename)[0] + '_' + str(j) + '.json'
    dest_file = open(output_dir + '/' + dest_name, 'w')
    for line in source_file:
        dest_file.write(line)
        i += 1
        if i == max_lines:
            j += 1
            i=0
            
            # Wrap up the current file, hacking off the trailing comma and adding a closing bracket.
            dest_file.seek(-2, os.SEEK_END)
            dest_file.truncate()
            dest_file.write(']')
            dest_file.close()
            
            # Start a new file.
            dest_name = os.path.splitext(filename)[0] + '_' + str(j) + '.json'
            dest_file = open(output_dir + '/' + dest_name, 'w')
            dest_file.write('[')
    
    source_file.close()
    
    # Delete the original JSON file, now that we have it in pieces.
    os.remove(output_dir + '/' + filename)

# Calculate the checksum digit for a given corporate ID
def checksum(corp_id):
    
    if len(corp_id) != 7:
        return False
    
    if corp_id[:1].isalpha():
        corp_id = '8' + corp_id[1:]
    
    # Rearrange the digits
    corp_id = corp_id[:1] + corp_id[6:7] + corp_id[1:-1]
    
    # Add up the odd digits.
    checksum = int(corp_id[:1]) + int(corp_id[2:3]) + int(corp_id[4:5]) + int(corp_id[6:7]);
    
    # Multiple each even digit by two; if the result is 2 digits, add them to each other.
    pos = [2, 4, 6]
    for even in pos:
        tmp = int(corp_id[even-1:even]) * 2
        if tmp > 9:
            tmp = int(tmp[:1]) + int(tmp[:-1])
        checksum = int(checksum) + tmp
    
    # Subtract the number from 100 and return the least significant digit.
    checksum = 100 - checksum
    checksum = str(checksum)
    checksum = checksum[1:2]
    
    return checksum

if __name__ == "__main__":
    main()
