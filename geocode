#!/usr/bin/env python
# -*- coding: utf-8 -*-

import csvkit
import os
import errno
import glob
import sys
import time
import math
import sqlite3
import urllib
import urllib2
import json

# Specify the file and the columnar start and end. Realistically, we should be pulling this data out
# of the YAML table maps, since we specify which collections of fields are address fields. That
# would obviate the need to specify filenames or column ranges.
input_file = "output/2_corporate.csv" #9-13
file_column_start = 9
file_column_end = 13
#input_file = "output/3_lp.csv" #10-14
#input_file = "output/9_llc.csv" #9-13

# We keep track of how many consecutive errors the API throws, to stop if there are too many.
api_error_count = 0

# Note the name of the API that we're using to geocode this data. (We store it in the database.)
source_api = 'VITA'

def main():
    
   # Make sure that we can connect to the database.
    try:
        db = sqlite3.connect('addresses.db')
    except sqlite3.error, e:
        print "Count not connect to SQLite, with error %s:" % e.args[0]
        sys.exit(1)
    
    # Create a SQLite cursor.
    cursor = db.cursor()
    
    # See if the addresses table already exists.
    cursor.execute("SELECT 1 FROM sqlite_master WHERE type='table' AND name='addresses'")
    exists = cursor.fetchone()
    
    # If the addresses table does not exist, try to retrieve it from a remote cache.
    if exists is None:

        from urllib2 import Request, urlopen, HTTPError
        url = 'https://s3.amazonaws.com/virginia-business/addresses.db'
        req = Request(url)
    
        try:
            f=urlopen(req)
            print "Downloading data from " + url
            local_file = open('current.zip', 'w')
            local_file.write(f.read())
            local_file.close()

        # If the addresses table cannot be retrieved from a remote cache, create it.
        except HTTPError as e:
        
            cursor.execute("CREATE TABLE addresses(address_hash TEXT PRIMARY KEY NOT NULL, "
            + "address_cleaned TEXT, latitude INTEGER, longitude INTEGER, date INTEGER, source TEXT)")
            db.commit()
    
    # Start reading from our input file.
    with open(input_file, 'rb') as csvfile:
    
        # Iterate through the lines in our input file.
        csvreader = csvkit.DictReader(csvfile)
        for record in csvreader:
            
            # Skip records without addresses.
            if record['street_1'] == '' and record['street_2'] == '':
                continue
            
            # Skip records outside of Virginia, since that's all our geocoder can handle.
            if record['state'] != 'VA':
                continue
            
            # Note the time at which we started reading this record, to make sure that we don't hit
            # the API more than per second.
            start_time = time.time()
            
            # Collapse our address columns and save that as a hash, to save in the database.
            address_hash = hash(record['street_1'] + "," + record['street_2'] + "," + \
                record['city'] + "," + record['state'] + "," + record['zip'])
    
            # if the hashed, combined string is already stored in the database, skip to the next
            # line
        
            # Remove quotation marks from street addresses.
            record['street_1'] = record['street_1'].replace('"', '')
            record['street_2'] = record['street_2'].replace('"', '')
        
            # If either address field starts with "C/O" or "ATTN:" drop it.
            if record['street_1'][:4].lower() == 'c/o ':
                del record['street_1']
            if record['street_2'][:4].lower() == 'c/o ':
                del record['street_2']
            if record['street_1'][:6].lower() == 'attn: ':
                del record['street_1']
            if record['street_2'][:6].lower() == 'attn: ':
                del record['street_2']
            
            # If either address 1 or address 2 contains "P[\.?][\s*])O[\.?] BOX" (or its variants),
            # and the other field isn't blank, then drop the PO BOX field.
            
            # If either address 1 or address 2 is blank, drop it.
            if record['street_1'] == '':
                del record['street_1']
            if record['street_2'] == '':
                del record['street_2']

            # Encode each field as a key/value pair, for our query, Key=[Value], combining address 1
            # and 2 (if both fields exist) into a single field.
            url_parameters = {}
            try:
                url_parameters['Street'] = record['street_1'] + ", " + record['street_2']
            except:
                try:
                    url_parameters['Street'] = record['street_1']
                except:
                    try:
                        url_parameters['Street'] = record['street_2']
                    except:
                        pass
            url_parameters['City'] = record['city']
            url_parameters['State'] = record['state']
            url_parameters['ZIP'] = record['zip']
            url_parameters['maxLocations'] = '1'
            url_parameters['f'] = 'json'
            url_parameters = urllib.urlencode(url_parameters)
            print url_parameters
            
            # Assemble the URL.
            request_url = 'http://gismaps.vita.virginia.gov/arcgis/rest/services/Geocoding/VGIN_Composite_Locator/GeocodeServer/findAddressCandidates?'
                + url_parameters
            
            # Fetch that URL.
            req = urllib2.Request(request_url)
            
            try:
                response = urllib2.urlopen(req)
            
            # If the URL request fails, increment our error count and skip to the next record.
            except HTTPError as e:
                api_error_count += 1
                continue
            
            # Because the request succeeded, reset our our error count to zero.
            api_error_count = 0
            
            # Grab the JSON response from the API.
            address_data = response.read()
            
            # If the top result score is below 95, or the address is just a ZIP (indicating that
            # we're getting the ZIP centroid), then record this as a failure and skip to the next
            # line.
                
            # Convert the coordinates to proper coordinate format.
                
            # Save the improved address.
                
            # Insert the hashed address, cleaned address, latitude, longitude, timestamp, and source
            # into the database.
            #cursor.execute("INSERT INTO addresses VALUES(" + address_hash + ", " + address_cleaned
            #    + ", " + latitude + ", " + longitude + ", " + time.time() + ", " + source_api + " )")
            
            # Pause long enough that 1 second has elapsed for this loop, to avoid hammering the API.
            elapsed_time = time.time() - start_time
            if (elapsed_time < 1):
                time.sleep(1 - elapsed_time)

    # Close our database connection.
    db.close()

if __name__ == "__main__":
     main()
